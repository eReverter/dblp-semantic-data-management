{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DBLP:\n",
    "    def __init__(self, path):\n",
    "        # Main\n",
    "        self.papers = pd.read_csv(f'{path}/papers.csv', sep=',')\n",
    "\n",
    "        # RDBS\n",
    "        self.authors = pd.read_csv(f'{path}/authors.csv', sep=',')\n",
    "        self.volumes = pd.read_csv(f'{path}/volumes.csv', sep=',')\n",
    "        self.editions = pd.read_csv(f'{path}/editions.csv', sep=',')\n",
    "        self.journals = pd.read_csv(f'{path}/journals.csv', sep=',')\n",
    "        self.conferences = pd.read_csv(f'{path}/conferences.csv', sep=',')\n",
    "        self.reviewers = pd.read_csv(f'{path}/reviewers.csv', sep=',')\n",
    "        self.reviews = pd.read_csv(f'{path}/reviews.csv', sep=',')\n",
    "        self.keywords = pd.read_csv(f'{path}/keywords.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp = DBLP('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pages</th>\n",
       "      <th>title</th>\n",
       "      <th>author_ids</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>keyword_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49-61</td>\n",
       "      <td>Estimating Traffic Disruption Patterns with Vo...</td>\n",
       "      <td>2685|70|3102|76|1426</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, we present an approach to devel...</td>\n",
       "      <td>jvlywkquewps</td>\n",
       "      <td>43|16|17|2|25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28-34</td>\n",
       "      <td>A Speaker-aware Parallel Hierarchical Attentiv...</td>\n",
       "      <td>1326|1181|136|1158</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, we present an empirical analysi...</td>\n",
       "      <td>ffthtdqohkxj</td>\n",
       "      <td>29|33|28|26|43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26-43</td>\n",
       "      <td>Robust Dialog State Tracking for Large Ontolog...</td>\n",
       "      <td>228|501|230|503|1692</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, we present an approach to devel...</td>\n",
       "      <td>yfwkbkvjolvv</td>\n",
       "      <td>30|10|11|5|9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23-30</td>\n",
       "      <td>From compression to compressed sensing</td>\n",
       "      <td>1716|628</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, we show that a simple system th...</td>\n",
       "      <td>lomhkwmufiws</td>\n",
       "      <td>3|17|21|2|10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10668-10688</td>\n",
       "      <td>Using Class Probabilities to Map Gradual Trans...</td>\n",
       "      <td>1155|1967|1866|1917|1859|1159</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this paper, we propose three proposed hypot...</td>\n",
       "      <td>geohcjlqsdai</td>\n",
       "      <td>35|44|14|5|39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        pages                                              title  \\\n",
       "0   0        49-61  Estimating Traffic Disruption Patterns with Vo...   \n",
       "1   1        28-34  A Speaker-aware Parallel Hierarchical Attentiv...   \n",
       "2   2        26-43  Robust Dialog State Tracking for Large Ontolog...   \n",
       "3   3        23-30             From compression to compressed sensing   \n",
       "4   4  10668-10688  Using Class Probabilities to Map Gradual Trans...   \n",
       "\n",
       "                      author_ids  volume_id  edition_id  \\\n",
       "0           2685|70|3102|76|1426       13.0         NaN   \n",
       "1             1326|1181|136|1158        4.0         NaN   \n",
       "2           228|501|230|503|1692       93.0         NaN   \n",
       "3                       1716|628       98.0         NaN   \n",
       "4  1155|1967|1866|1917|1859|1159       54.0         NaN   \n",
       "\n",
       "                                            abstract           doi  \\\n",
       "0  In this paper, we present an approach to devel...  jvlywkquewps   \n",
       "1  In this paper, we present an empirical analysi...  ffthtdqohkxj   \n",
       "2  In this paper, we present an approach to devel...  yfwkbkvjolvv   \n",
       "3  In this paper, we show that a simple system th...  lomhkwmufiws   \n",
       "4  In this paper, we propose three proposed hypot...  geohcjlqsdai   \n",
       "\n",
       "      keyword_ids  \n",
       "0   43|16|17|2|25  \n",
       "1  29|33|28|26|43  \n",
       "2    30|10|11|5|9  \n",
       "3    3|17|21|2|10  \n",
       "4   35|44|14|5|39  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp.papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na(df):\n",
    "    return df.fillna(-1)\n",
    "\n",
    "def convert_to_int(df):\n",
    "    numeric_columns = df.select_dtypes(include=[float, int]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_key in vars(dblp):\n",
    "    setattr(dblp, df_key, impute_na(getattr(dblp, df_key)))\n",
    "    setattr(dblp, df_key, convert_to_int(getattr(dblp, df_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pages</th>\n",
       "      <th>title</th>\n",
       "      <th>author_ids</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>edition_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>keyword_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49-61</td>\n",
       "      <td>Estimating Traffic Disruption Patterns with Vo...</td>\n",
       "      <td>2685|70|3102|76|1426</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>In this paper, we present an approach to devel...</td>\n",
       "      <td>jvlywkquewps</td>\n",
       "      <td>43|16|17|2|25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28-34</td>\n",
       "      <td>A Speaker-aware Parallel Hierarchical Attentiv...</td>\n",
       "      <td>1326|1181|136|1158</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>In this paper, we present an empirical analysi...</td>\n",
       "      <td>ffthtdqohkxj</td>\n",
       "      <td>29|33|28|26|43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26-43</td>\n",
       "      <td>Robust Dialog State Tracking for Large Ontolog...</td>\n",
       "      <td>228|501|230|503|1692</td>\n",
       "      <td>93</td>\n",
       "      <td>-1</td>\n",
       "      <td>In this paper, we present an approach to devel...</td>\n",
       "      <td>yfwkbkvjolvv</td>\n",
       "      <td>30|10|11|5|9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23-30</td>\n",
       "      <td>From compression to compressed sensing</td>\n",
       "      <td>1716|628</td>\n",
       "      <td>98</td>\n",
       "      <td>-1</td>\n",
       "      <td>In this paper, we show that a simple system th...</td>\n",
       "      <td>lomhkwmufiws</td>\n",
       "      <td>3|17|21|2|10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10668-10688</td>\n",
       "      <td>Using Class Probabilities to Map Gradual Trans...</td>\n",
       "      <td>1155|1967|1866|1917|1859|1159</td>\n",
       "      <td>54</td>\n",
       "      <td>-1</td>\n",
       "      <td>In this paper, we propose three proposed hypot...</td>\n",
       "      <td>geohcjlqsdai</td>\n",
       "      <td>35|44|14|5|39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        pages                                              title  \\\n",
       "0   0        49-61  Estimating Traffic Disruption Patterns with Vo...   \n",
       "1   1        28-34  A Speaker-aware Parallel Hierarchical Attentiv...   \n",
       "2   2        26-43  Robust Dialog State Tracking for Large Ontolog...   \n",
       "3   3        23-30             From compression to compressed sensing   \n",
       "4   4  10668-10688  Using Class Probabilities to Map Gradual Trans...   \n",
       "\n",
       "                      author_ids  volume_id  edition_id  \\\n",
       "0           2685|70|3102|76|1426         13          -1   \n",
       "1             1326|1181|136|1158          4          -1   \n",
       "2           228|501|230|503|1692         93          -1   \n",
       "3                       1716|628         98          -1   \n",
       "4  1155|1967|1866|1917|1859|1159         54          -1   \n",
       "\n",
       "                                            abstract           doi  \\\n",
       "0  In this paper, we present an approach to devel...  jvlywkquewps   \n",
       "1  In this paper, we present an empirical analysi...  ffthtdqohkxj   \n",
       "2  In this paper, we present an approach to devel...  yfwkbkvjolvv   \n",
       "3  In this paper, we show that a simple system th...  lomhkwmufiws   \n",
       "4  In this paper, we propose three proposed hypot...  geohcjlqsdai   \n",
       "\n",
       "      keyword_ids  \n",
       "0   43|16|17|2|25  \n",
       "1  29|33|28|26|43  \n",
       "2    30|10|11|5|9  \n",
       "3    3|17|21|2|10  \n",
       "4   35|44|14|5|39  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dblp.papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from functools import partial\n",
    "\n",
    "def assign_category(observation, ref_col, available_categories, col_name='category'):\n",
    "    # Get hash of the observation id\n",
    "    hash_id = hash(observation[ref_col])\n",
    "\n",
    "    # Get the index of the hash in the available subtypes\n",
    "    index = hash_id % len(available_categories)\n",
    "\n",
    "    # Append the subtype to the observation\n",
    "    observation[col_name] = available_categories[index]\n",
    "\n",
    "    return observation\n",
    "\n",
    "def random_string(length):\n",
    "    import random\n",
    "    import string\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(length))\n",
    "\n",
    "def add_extra_attribute(observation, length=10, col_name='attribute'):\n",
    "    observation[col_name] = random_string(length)\n",
    "    return observation\n",
    "\n",
    "# Impute missing values wrt the assignment\n",
    "assign_paper_subtype_conf = partial(assign_category, ref_col='edition_id', available_categories=['DemoPaper', 'ShortPaper', 'FullPaper', 'Poster'])\n",
    "assign_paper_subtype_jour = partial(assign_category, ref_col='volume_id', available_categories=['DemoPaper', 'ShortPaper', 'FullPaper'])\n",
    "assign_conf_subtype = partial(assign_category, ref_col='id', available_categories=['Workshop', 'Symposium', 'ExpertGroup'])\n",
    "assign_conf_related = partial(assign_category, ref_col='id', available_categories=[0, 1, 5, 6], col_name='relatedTo')\n",
    "\n",
    "dblp.papers = dblp.papers.apply(lambda x: assign_paper_subtype_jour(x) if x.volume_id != -1 else assign_paper_subtype_conf(x) , axis=1)\n",
    "dblp.conferences = dblp.conferences.apply(lambda x: assign_conf_subtype(x), axis=1)\n",
    "dblp.conferences = dblp.conferences.apply(lambda x: assign_conf_related(x), axis=1)\n",
    "\n",
    "dblp.papers = dblp.papers.apply(lambda x: add_extra_attribute(x, length=10, col_name='subpaper_attr'), axis=1)\n",
    "dblp.papers = dblp.papers.apply(lambda x: add_extra_attribute(x, length=10, col_name='paper_type_attr'), axis=1)\n",
    "\n",
    "dblp.conferences = dblp.conferences.apply(lambda x: add_extra_attribute(x, length=10, col_name='subconf_attr'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N8dcb7b6336094bfbbb48f61797222312 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from rdflib import Graph, Literal, BNode, Namespace, RDF, URIRef\n",
    "from rdflib.namespace import XSD\n",
    "import numpy as np\n",
    "\n",
    "# Create a namespace for your ontology\n",
    "n = Namespace(\"http://www.sdm.com/ontology#\")\n",
    "\n",
    "# Create a RDF graph\n",
    "g = Graph()\n",
    "\n",
    "# Load the data from the df into the ontology ABOX\n",
    "\n",
    "## Add chairs and editors (one for ease)\n",
    "chair = URIRef(n + 'chair')\n",
    "g.add((chair, RDF.type, n.Chair))\n",
    "g.add((chair, n.role, Literal('Chair', datatype=XSD.string)))\n",
    "\n",
    "editor = URIRef(n + 'editor')\n",
    "g.add((editor, RDF.type, n.Editor))\n",
    "g.add((editor, n.house, Literal('Editor', datatype=XSD.string)))\n",
    "\n",
    "## Load authors and reviewers\n",
    "for _, row in dblp.authors.iterrows():\n",
    "    author = URIRef(n + 'author' + str(row['id']))\n",
    "    reviewer = URIRef(n + 'reviewer' + str(row['id']))\n",
    "    g.add((author, RDF.type, n.Author))\n",
    "    g.add((reviewer, RDF.type, n.Reviewer))\n",
    "    g.add((author, n.hindex, Literal(np.random.random(), datatype=XSD.float)))\n",
    "    g.add((reviewer, n.experience, Literal(np.random.randint(1, 100), datatype=XSD.integer)))\n",
    "    g.add((author, n.name, Literal(row['author'], datatype=XSD.string))) # OWL takes care of assigning the datatypes to the proper subclass\n",
    "    g.add((reviewer, n.name, Literal(row['author'], datatype=XSD.string)))\n",
    "\n",
    "    g.add((chair, n.assigns, reviewer)) # Just assume all reviers are assigned by both at some point (boring assignment)\n",
    "    g.add((editor, n.assigns, reviewer))\n",
    "\n",
    "## Load keywords\n",
    "for _, row in dblp.keywords.iterrows():\n",
    "    keyword = URIRef(n + 'area' + str(row['id']))\n",
    "    g.add((keyword, RDF.type, n.KnowledgeArea))\n",
    "    g.add((keyword, n.name, Literal(row['keyword'], datatype=XSD.string)))\n",
    "\n",
    "## Load journals\n",
    "for _, row in dblp.journals.iterrows():\n",
    "    journal = URIRef(n + 'journal' + str(row['id']))\n",
    "    g.add((journal, RDF.type, n.Journal))\n",
    "    g.add((journal, n.name, Literal(row['journal'], datatype=XSD.string)))\n",
    "    g.add((journal, n.publisher, Literal('IEEE', datatype=XSD.string)))\n",
    "    g.add((journal, n.relatedTo, URIRef(n + 'area' + str(0))))\n",
    "\n",
    "## Load conferences\n",
    "for _, row in dblp.conferences.iterrows():\n",
    "    conference = URIRef(n + 'conference' + str(row['id']))\n",
    "    if row['category'] == 'Workshop':\n",
    "        g.add((conference, RDF.type, n.Workshop))\n",
    "        g.add((conference, n.organizers, Literal(row['subconf_attr'], datatype=XSD.string)))\n",
    "    elif row['category'] == 'Symposium':\n",
    "        g.add((conference, RDF.type, n.Symposium))\n",
    "        g.add((conference, n.program, Literal(row['subconf_attr'], datatype=XSD.string)))\n",
    "    elif row['category'] == 'ExpertGroup':\n",
    "        g.add((conference, RDF.type, n.ExpertGroup))\n",
    "        g.add((conference, n.domain, Literal(row['subconf_attr'], datatype=XSD.string)))\n",
    "\n",
    "    g.add((conference, n.name, Literal(row['conference'], datatype=XSD.string)))\n",
    "    g.add((conference, n.publisher, Literal('IEEE', datatype=XSD.string)))\n",
    "    g.add((conference, n.relatedTo, URIRef(n + 'area' + str(row['relatedTo']))))\n",
    "\n",
    "## Load editions\n",
    "for _, row in dblp.editions.iterrows():\n",
    "    edition = URIRef(n + 'edition' +  str(row['id']))\n",
    "    g.add((edition, RDF.type, n.ConferenceProceedings))\n",
    "\n",
    "    g.add((edition, n.heldIn, URIRef(n + 'conference' + str(int(row['conference_id'])))))\n",
    "    g.add((edition, n.edition, Literal(row['city']+str(row['year']), datatype=XSD.string)))\n",
    "\n",
    "## Load volumes\n",
    "for _, row in dblp.volumes.iterrows():\n",
    "    volume = URIRef(n + 'volume' +  str(row['id']))\n",
    "    g.add((volume, RDF.type, n.JournalVolume))\n",
    "\n",
    "    g.add((volume, n.heldIn, URIRef(n + 'journal' + str(int(row['journal_id'])))))\n",
    "    g.add((volume, n.volume, Literal(row['volume'], datatype=XSD.string)))\n",
    "\n",
    "## Load papers\n",
    "for _, row in dblp.papers.iterrows():\n",
    "    paper = URIRef(n + 'paper' + str(row['id']))\n",
    "    if row['category'] == 'DemoPaper':\n",
    "        g.add((paper, RDF.type, n.DemoPaper))\n",
    "        g.add((paper, n.demo, Literal(row['subpaper_attr'], datatype=XSD.string)))\n",
    "        g.add((paper, n.background, Literal(row['paper_type_attr'], datatype=XSD.string)))\n",
    "    elif row['category'] == 'ShortPaper':\n",
    "        g.add((paper, RDF.type, n.ShortPaper))\n",
    "        g.add((paper, n.conciseness, Literal(row['subpaper_attr'], datatype=XSD.string)))\n",
    "        g.add((paper, n.background, Literal(row['paper_type_attr'], datatype=XSD.string)))\n",
    "    elif row['category'] == 'FullPaper':\n",
    "        g.add((paper, RDF.type, n.FullPaper))\n",
    "        g.add((paper, n.discussion, Literal(row['subpaper_attr'], datatype=XSD.string)))\n",
    "        g.add((paper, n.background, Literal(row['paper_type_attr'], datatype=XSD.string)))\n",
    "    elif row['category'] == 'Poster':\n",
    "        g.add((paper, RDF.type, n.Poster))\n",
    "        g.add((paper, n.dimensions, Literal(row['subpaper_attr'], datatype=XSD.string)))\n",
    "        g.add((paper, n.track, Literal(row['paper_type_attr'], datatype=XSD.string)))\n",
    "    \n",
    "    g.add((paper, n.title, Literal(row['title'], datatype=XSD.string)))\n",
    "    g.add((paper, n.abstract, Literal(row['abstract'], datatype=XSD.string)))\n",
    "\n",
    "    # Link with authors\n",
    "    for author in row['author_ids'].split('|'):\n",
    "        g.add((paper, n.authoredBy, URIRef(n + 'author' + author)))\n",
    "\n",
    "    # Link with areas\n",
    "    for keyword in row['keyword_ids'].split('|'):\n",
    "        g.add((paper, n.relatedTo, URIRef(n + 'area' + keyword)))\n",
    "\n",
    "    # Link with publications\n",
    "    if row['volume_id'] != -1:\n",
    "        g.add((paper, n.publishedIn, URIRef(n + 'volume' +  str(row['volume_id']))))\n",
    "\n",
    "    elif row['edition_id'] != -1:\n",
    "        g.add((paper, n.publishedIn, URIRef(n + 'edition' +  str(row['edition_id']))))\n",
    "\n",
    "## Load reviews\n",
    "### Should reviewers and authors be different entities?\n",
    "for i, row in dblp.reviews.iterrows():\n",
    "    review = URIRef(n + 'review' + str(i))\n",
    "    g.add((review, RDF.type, n.Review))\n",
    "    g.add((review, n.ReviewDecision, Literal(row['decision'], datatype=XSD.string)))\n",
    "    g.add((review, n.ReviewText, Literal(row['content'], datatype=XSD.string)))\n",
    "\n",
    "    g.add((review, n.reviewedPaper, URIRef(n + 'paper' + str(row['article_id']))))\n",
    "    g.add((review, n.submitedBy, URIRef(n + 'author' + str(row['author_id']))))\n",
    "\n",
    "\n",
    "# Serialize the graph in RDF/XML format\n",
    "g.serialize(destination='abox.nt', format='nt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N77a02e792b57471da5e16bb7d4151b8b (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.parse('tbox_grafo.owl', format='xml')\n",
    "f.parse('abox.nt', format='nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61341\n"
     ]
    }
   ],
   "source": [
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N77a02e792b57471da5e16bb7d4151b8b (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.serialize(destination='linked_data.nt', format='nt', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
